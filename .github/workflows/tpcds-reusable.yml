#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

name: TPC-DS Reusable

on:
  workflow_call:
    inputs:
      sparkver:
        description: 'Maven profile id to resolve sparkVersion (e.g., spark-3.5)'
        required: true
        type: string
      sparktests:
        description: 'Whether to enable spark correctness tests'
        required: false
        type: string
        default: 'false'
      hadoop-profile:
        description: 'Hadoop profile (e.g., hadoop2.7, hadoop3)'
        required: true
        type: string
        default: ''
      javaver:
        description: 'Optional Java version'
        required: false
        type: string
        default: '8'
      scalaver:
        description: 'Optional Scala version'
        required: false
        type: string
        default: '2.12'
      celebornver:
        description: 'Optional Celeborn version'
        required: false
        type: string
        default: ''
      unifflever:
        description: 'Optional Uniffle version'
        required: false
        type: string
        default: ''
      hadoopver:
        description: 'Optional Hadoop version'
        required: false
        type: string
        default: ''
      extrasparkconf:
        description: 'Optional extra Spark conf to pass'
        required: false
        type: string
        default: ''
      queries:
        description: 'Optional list of queries to run'
        required: false
        type: string
        default: |
          [
            "q1,q2,q3,q4,q5,q6,q7,q8,q9",
            "q10,q11,q12,q13,q14a,q14b,q15,q16,q17,q18,q19",
            "q20,q21,q22,q23a,q23b,q24a,q24b,q25,q26,q27,q28,q29",
            "q30,q31,q32,q33,q34,q35,q36,q37,q38,q39a,q39b",
            "q40,q41,q42,q43,q44,q45,q46,q47,q48,q49",
            "q50,q51,q52,q53,q54,q55,q56,q57,q58,q59",
            "q60,q61,q62,q63,q64,q65,q66,q67,q68,q69",
            "q70,q71,q72,q73,q74,q75,q76,q77,q78,q79",
            "q80,q81,q82,q83,q84,q85,q86,q87,q88,q89",
            "q90,q91,q92,q93,q94,q95,q96,q97,q98,q99"
          ]

env:
  APACHE_MIRROR: "https://www.apache.org/dyn/closer.lua"
  MIRROR_URL_QUERY: "?action=download"
  WGET_OPTS: "--tries=3 --timeout=30 -c"

jobs:
  build-auron-jar:
    name: Build Auron JAR
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout Auron
        uses: actions/checkout@v6
        with:
          submodules: recursive

      - name: Setup Java and Maven cache
        uses: actions/setup-java@v5
        with:
          distribution: 'adopt-hotspot'
          java-version: ${{ inputs.javaver }}
          cache: 'maven'

      - name: Setup protoc
        uses: arduino/setup-protoc@v3
        with:
          version: "21.7"
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Rust toolchain
        uses:  ./.github/actions/setup-rust-toolchain
        with:
          rustflags: --allow warnings -C target-feature=+aes
          components:
            cargo

      - name: Build auron (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, JDK ${{ inputs.javaver }})
        run: |
          rm -f .build-checksum_*.cache
          
          SPARK_NUMBER="${{ inputs.sparkver }}"
          SPARK_NUMBER="${SPARK_NUMBER#spark-}"
          
          CELEBORN_NUMBER="${{ inputs.celebornver }}"
          if [ -n "${{ inputs.celebornver }}" ]; then
            CELEBORN_NUMBER="${CELEBORN_NUMBER#celeborn-}"
          fi
          
          UNIFFLE_NUMBER="${{ inputs.unifflever }}"
          if [ -n "${{ inputs.unifflever }}" ]; then
            UNIFFLE_NUMBER="${UNIFFLE_NUMBER#uniffle-}"
          fi
          
          CMD="./auron-build.sh --pre --sparkver $SPARK_NUMBER --scalaver ${{ inputs.scalaver }} --skiptests false"
          if [ -n "${{ inputs.celebornver }}" ]; then
            CMD="$CMD --celeborn $CELEBORN_NUMBER"
          fi
          if [ -n "${{ inputs.unifflever }}" ]; then
            CMD="$CMD --uniffle $UNIFFLE_NUMBER"
          fi

          SPARK_TESTS="${{ inputs.sparktests }}"
          if [[ "$SPARK_TESTS" == "true" ]]; then
            CMD="$CMD --sparktests true"
          fi

          echo "Running: $CMD"
          exec $CMD

      - name: Upload unit test reports
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: unit-tests-reports-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver }}${{ inputs.celebornver && format('-{0}', inputs.celebornver) || '' }}${{ inputs.unifflever && format('-{0}', inputs.unifflever) || '' }}
          path: "**/target/surefire-reports/*.xml"
      - name: Upload unit tests log
        if: failure()
        uses: actions/upload-artifact@v6
        with:
          name: unit-tests-logs-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver }}${{ inputs.celebornver && format('-{0}', inputs.celebornver) || '' }}${{ inputs.unifflever && format('-{0}', inputs.unifflever) || '' }}
          path: "**/target/unit-tests.log"
      - name: Upload auron (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, JDK ${{ inputs.javaver }})
        uses: actions/upload-artifact@v6
        with:
          name: >
            auron-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver 
            }}${{ inputs.celebornver && format('-{0}', inputs.celebornver) || '' 
            }}${{ inputs.unifflever && format('-{0}', inputs.unifflever) || '' }}
          path: target/auron-${{ inputs.sparkver }}_${{ inputs.scalaver }}-pre-*.jar
          overwrite: true

      - name: Build auron-it (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, JDK ${{ inputs.javaver }})
        run: |
          cd dev/auron-it
          ${{ github.workspace }}/build/mvn package -P${{ inputs.sparkver }} -Pscala-${{ inputs.scalaver }}
      - name: Upload auron it (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, JDK ${{ inputs.javaver }})
        uses: actions/upload-artifact@v6
        with:
          name: auron-it-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver }}
          path: dev/auron-it/target/auron-it-${{ inputs.sparkver }}_${{ inputs.scalaver }}*-jar-with-dependencies.jar
          overwrite: true

  run-tpcds-test:
    name: Run TPC-DS test ${{ matrix.query }}
    needs: [build-auron-jar]
    runs-on: ubuntu-24.04
    strategy:
      fail-fast: false
      matrix:
        query: ${{ fromJson(inputs.queries) }}
    steps:
      - name: Checkout Auron
        uses: actions/checkout@v6

      - name: Get dependency version from pom
        id: get-dependency-version
        run: |
          SPARK_VERSION=$(./build/mvn help:evaluate -N -Dexpression=sparkVersion -P${{ inputs.sparkver }} -q -DforceStdout)
          echo "Detected Spark VERSION: $SPARK_VERSION"
          echo "sparkversion=$SPARK_VERSION" >> $GITHUB_OUTPUT

          if [ -n "${{ inputs.celebornver }}" ]; then
            CELEBORN_VERSION=$(./build/mvn help:evaluate -N -Dexpression=celebornVersion -P${{ inputs.sparkver }} -P${{ inputs.celebornver }} -q -DforceStdout)
            echo "Detected Celeborn VERSION: $CELEBORN_VERSION"
            echo "celebornversion=$CELEBORN_VERSION" >> $GITHUB_OUTPUT
          fi

          if [ -n "${{ inputs.unifflever }}" ]; then
            UNIFFLE_VERSION=$(./build/mvn help:evaluate -N -Dexpression=uniffleVersion -P${{ inputs.sparkver }} -P${{ inputs.unifflever }} -q -DforceStdout)
            echo "Detected Uniffle VERSION: $UNIFFLE_VERSION"
            echo "uniffleversion=$UNIFFLE_VERSION" >> $GITHUB_OUTPUT
          fi

      - name: Cache Spark (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, ${{ inputs.hadoop-profile }})
        uses: actions/cache@v5
        id: cache-spark-bin
        with:
          path: spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}
          key: spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}-${{ inputs.hadoop-profile }}

      - name: Setup Spark (Spark ${{ inputs.sparkver }}, Scala ${{ inputs.scalaver }}, ${{ inputs.hadoop-profile }})
        id: setup-spark-bin
        if: steps.cache-spark-bin.outputs.cache-hit != 'true'
        run: |
          SPARK_PATH="spark/spark-${{ steps.get-dependency-version.outputs.sparkversion }}"
          if [[ ${{ inputs.scalaver }} = "2.13" && "${{ inputs.sparkver }}" != "spark-4.0" && "${{ inputs.sparkver }}" != "spark-4.1" ]]; then
            SPARK_FILE="spark-${{ steps.get-dependency-version.outputs.sparkversion }}-bin-${{ inputs.hadoop-profile }}-scala${{ inputs.scalaver }}.tgz"
          else
            SPARK_FILE="spark-${{ steps.get-dependency-version.outputs.sparkversion }}-bin-${{ inputs.hadoop-profile }}.tgz"
          fi
          SPARK_URL="${APACHE_MIRROR}/${SPARK_PATH}/${SPARK_FILE}${MIRROR_URL_QUERY}"
          
          wget ${WGET_OPTS} ${SPARK_URL} -O "$SPARK_FILE"
          mkdir -p spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}
          cd spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }} && tar -xf ../spark-*.tgz --strip-component=1

      - name: Download Auron JAR
        uses: actions/download-artifact@v7
        with:
          name: >
            auron-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver 
            }}${{ inputs.celebornver && format('-{0}', inputs.celebornver) || '' 
            }}${{ inputs.unifflever && format('-{0}', inputs.unifflever) || '' }}

      - name: Download auron-it JAR
        uses: actions/download-artifact@v7
        with:
          name: auron-it-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver }}

      - name: Checkout TPC-DS Data
        uses: actions/checkout@v6
        with:
          repository: auron-project/tpcds_1g
          path: dev/tpcds_1g

      - name: Install Auron JAR
        run: |
          ls -la
          jar=$(ls -1 auron-${{ inputs.sparkver }}_${{ inputs.scalaver }}*.jar | head -n1)
          [ -n "$jar" ] || { echo "No jar matched: auron-${{ inputs.sparkver }}_${{ inputs.scalaver }}*.jar"; exit 1; }
          echo "AURON_SPARK_JAR=$jar" >> "$GITHUB_ENV"
          cp "$jar" spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}/jars/

      - name: Setup Java and Maven cache
        uses: actions/setup-java@v5
        with:
          distribution: 'adopt-hotspot'
          java-version: ${{ inputs.javaver }}
          cache: 'maven'

      - name: Install auron-it JAR
        run: |
          ls -la
          jar=$(ls -1 auron-it-*-jar-with-dependencies.jar | head -n1)
          [ -n "$jar" ] || { echo "No jar matched: auron-it-*-jar-with-dependencies.jar"; exit 1; }
          echo "AURON_IT_JAR=$jar" >> "$GITHUB_ENV"

      - name: Cache Celeborn-${{ steps.get-dependency-version.outputs.celebornversion }}
        uses: actions/cache@v5
        if: ${{ inputs.celebornver != '' }}
        id: cache-celeborn-bin
        with:
          path: celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }}
          key: celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }}

      - name: Setup Celeborn-${{ steps.get-dependency-version.outputs.celebornversion }}
        id: setup-celeborn-bin
        if: ${{ inputs.celebornver != '' && steps.cache-celeborn-bin.outputs.cache-hit != 'true' }}
        run: |
          CELEBORN_PATH="celeborn/celeborn-${{ steps.get-dependency-version.outputs.celebornversion }}"
          CELEBORN_FILE="apache-celeborn-${{ steps.get-dependency-version.outputs.celebornversion }}-bin.tgz"
          CELEBORN_URL="${APACHE_MIRROR}/${CELEBORN_PATH}/${CELEBORN_FILE}${MIRROR_URL_QUERY}"
          
          wget ${WGET_OPTS} ${CELEBORN_URL} -O "$CELEBORN_FILE" && \
          mkdir -p celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }} && \
          cd celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }} && tar -xf ../apache-celeborn-*.tgz --strip-component=1

      - name: Start Celeborn-${{ steps.get-dependency-version.outputs.celebornversion }}
        if: ${{ inputs.celebornver != '' }}
        run: |
          mkdir -p /tmp/rss/data && mkdir -p /tmp/rss/logs && \
          cd celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }} && \
          bash -c "echo -e 'CELEBORN_MASTER_MEMORY=4g\nCELEBORN_WORKER_MEMORY=4g\nCELEBORN_WORKER_OFFHEAP_MEMORY=8g\nCELEBORN_LOG_DIR=/tmp/rss/logs' > ./conf/celeborn-env.sh" && \
          bash -c "echo -e 'celeborn.worker.storage.dirs /tmp/rss\nceleborn.worker.storage.workingDir data\nceleborn.worker.commitFiles.threads 128\nceleborn.worker.sortPartition.threads 64' > ./conf/celeborn-defaults.conf" && \
          bash ./sbin/start-master.sh && \
          bash ./sbin/start-worker.sh

      - name: Install Celeborn JAR
        if: ${{ inputs.celebornver != '' }}
        run: |
          ls -la celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }}/spark
          cp celeborn-bin-${{ steps.get-dependency-version.outputs.celebornversion }}/spark/celeborn-client-spark-*_${{ inputs.scalaver }}-*.jar spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}/jars/

      - name: Cache Uniffle-${{ steps.get-dependency-version.outputs.uniffleversion }}
        uses: actions/cache@v5
        if: ${{ inputs.unifflever != '' }}
        id: cache-uniffle-bin
        with:
          path: uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }}
          key: uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }}

      - name: Setup Uniffle-${{ steps.get-dependency-version.outputs.uniffleversion }}
        id: setup-uniffle-bin
        if: ${{ inputs.unifflever != '' && steps.cache-uniffle-bin.outputs.cache-hit != 'true' }}
        run: |
          UNIFFLE_PATH="uniffle/${{ steps.get-dependency-version.outputs.uniffleversion }}"
          UNIFFLE_FILE="apache-uniffle-${{ steps.get-dependency-version.outputs.uniffleversion }}-bin.tar.gz"
          UNIFFLE_URL="${APACHE_MIRROR}/${UNIFFLE_PATH}/${UNIFFLE_FILE}${MIRROR_URL_QUERY}"

          wget ${WGET_OPTS} ${UNIFFLE_URL} -O "$UNIFFLE_FILE" && \
          mkdir -p uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }} && \
          tar -xf ./$UNIFFLE_FILE -C uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }} --strip-component=1

      - name: Cache hadoop-${{ inputs.hadoopver }}
        uses: actions/cache@v5
        if: ${{ inputs.hadoopver != '' }}
        id: cache-hadoop-bin
        with:
          path: hadoop-bin-${{ inputs.hadoopver }}
          key: hadoop-bin-${{ inputs.hadoopver }}

      - name: Setup hadoop-${{ inputs.hadoopver }}
        id: setup-hadoop-bin
        if: ${{ inputs.hadoopver != '' && steps.cache-hadoop-bin.outputs.cache-hit != 'true' }}
        run: |
          HADOOP_PATH="hadoop/common/hadoop-${{ inputs.hadoopver }}"
          HADOOP_FILE="hadoop-${{ inputs.hadoopver }}.tar.gz"
          HADOOP_URL="${APACHE_MIRROR}/${HADOOP_PATH}/${HADOOP_FILE}${MIRROR_URL_QUERY}"
          
          wget ${WGET_OPTS} ${HADOOP_URL} -O "$HADOOP_FILE" && \
          mkdir -p hadoop-bin-${{ inputs.hadoopver }} && \
          tar -xf ./$HADOOP_FILE -C hadoop-bin-${{ inputs.hadoopver }} --strip-component=1  && \
          ls -la hadoop-bin-${{ inputs.hadoopver }}

      - name: Start Uniffle-${{ steps.get-dependency-version.outputs.uniffleversion }}
        if: ${{ inputs.unifflever != '' }}
        run: |
          mkdir -p /tmp/rss/data && mkdir -p /tmp/rss/logs && \
          cd uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }} && \
          bash -c "echo -e 'XMX_SIZE=16g\nHADOOP_HOME=../hadoop-bin-${{ inputs.hadoopver }}\nRSS_LOG_DIR=/tmp/rss/logs' > ./conf/rss-env.sh" && \
          bash -c "echo -e 'rss.coordinator.shuffle.nodes.max 1\nrss.rpc.server.port 19999' > ./conf/coordinator.conf" && \
          bash -c "echo -e 'rss.server.app.expired.withoutHeartbeat 7200000\nrss.server.heartbeat.delay 3000\nrss.rpc.server.port 19997\nrss.rpc.server.type GRPC_NETTY\nrss.jetty.http.port 19996\nrss.server.netty.port 19995\nrss.storage.basePath /tmp/rss/data\nrss.storage.type MEMORY_LOCALFILE\nrss.coordinator.quorum localhost:19999\nrss.server.flush.thread.alive 10\nrss.server.single.buffer.flush.threshold 64m' > ./conf/server.conf" && \
          bash ./bin/start-coordinator.sh && bash ./bin/start-shuffle-server.sh

      - name: Install Uniffle JAR
        if: ${{ inputs.unifflever != '' }}
        run: |
          ls -la uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }}/jars/client/spark3/
          cp uniffle-bin-${{ steps.get-dependency-version.outputs.uniffleversion }}/jars/client/spark3/*.jar spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}/jars/

      - name: Run TPC-DS Query ${{ matrix.query }}
        env:
          RUST_LOG: ERROR
          RUST_BACKTRACE: 1
          SPARK_VERSION: ${{ inputs.sparkver }}
          SCALA_VERSION: ${{ inputs.scalaver }}
          SPARK_HOME: spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}
        run: |
          ls -la
          dev/auron-it/run-it.sh  \
            ${{ inputs.extrasparkconf }} \
            --type tpcds \
            --data-location dev/tpcds_1g \
            --query-filter ${{ matrix.query }} \
            --result-check \
            --plan-check

      - name: Upload RSS log
        if: ${{ failure() && (inputs.celebornver != '' || inputs.unifflever != '') }}
        uses: actions/upload-artifact@v6
        with:
          name: >
            rss-log-${{ inputs.sparkver }}_${{ inputs.scalaver }}-jdk-${{ inputs.javaver
            }}${{ inputs.celebornver && format('-{0}', inputs.celebornver) || ''
            }}${{ inputs.unifflever && format('-{0}', inputs.unifflever) || '' }}
          path: |
            /tmp/rss/logs/*

      - name: Clean up injected JARs to keep Spark bin cache pure
        if: steps.cache-spark-bin.outputs.cache-hit != 'true'
        run: |
          cd spark-bin-${{ inputs.sparkver }}_${{ inputs.scalaver }}/jars/
          rm -f auron-*${{ inputs.sparkver }}_${{ inputs.scalaver }}*.jar
          rm -f celeborn-client-spark-*_${{ inputs.scalaver }}-*.jar
          rm -f rss-client-spark*.jar
          ls -la ./

